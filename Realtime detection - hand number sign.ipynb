{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.;C:\\\\Users\\\\Playdata\\\\코딩\\\\ObjectDetection\\\\models;C:\\\\Users\\\\Playdata\\\\코딩\\\\ObjectDetection\\\\models\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYTHONPATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util, config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_CONFIG_PATH = os.path.join('model', 'pipeline.config')\n",
    "CHECK_POINT_PATH = os.path.join('model', 'checkpoint', 'ckpt-21')\n",
    "LABEL_MAP_FILE_PATH = os.path.join('labelmap', 'label_map.pbtxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x23b90642cc8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline.config에 맞춰서 추출한 모델을 바탕으로 모델을 생성\n",
    "\n",
    "# pipeline.config를 조회\n",
    "config = config_util.get_configs_from_pipeline_file(PIPELINE_CONFIG_PATH)\n",
    "# pipeline.config의 model 설정 정보를 넣어 모델 생성\n",
    "detection_model = model_builder.build(model_config=config['model'], is_training=False)\n",
    "\n",
    "# 모델에 학습시킨 checkpoint(weight)를 주입\n",
    "# checkpoint 조회\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(CHECK_POINT_PATH)).expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detection을 실행하는 함수\n",
    "\n",
    "# 순전파처리 함수에 @tf.function decorator를 선언하면 실행 속도가 빨라짐\n",
    "@tf.function\n",
    "def detect_func(image):\n",
    "    \"\"\"\n",
    "    매개변수로 object detection을 수행할 대상 image(Tensor)를 받아서 detection 처리\n",
    "    1. preprocessing(전처리): resize, normalization 작업\n",
    "    2. detection(inference-추론)\n",
    "    3. detection 결과를 post processing : Non Maximum Suppression\n",
    "    4. post processing한 결과 반환\n",
    "    \"\"\"\n",
    "    # 1. preprocessing\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    # 2. 추론\n",
    "    predict_dict = detection_model.predict(image, shapes)\n",
    "    # 3. post processing\n",
    "    result = detection_model.postprocess(predict_dict, shapes)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹캠으로부터 이미지를 받아서 추론한 결과를 화면에 보여주기\n",
    "cap = cv2.VideoCapture(0)\n",
    "# 웹캠의 WIRDTH/HEIGHT 조회\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "category_index = label_map_util.create_category_index_from_labelmap(LABEL_MAP_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()  #한 frame 읽기\n",
    "    if not ret:\n",
    "        print(\"이미지를 읽지 못 했습니다\")\n",
    "        break\n",
    "        \n",
    "    frame = cv2.flip(frame, 1)  #좌우 반전\n",
    "    \n",
    "    # BGR => RGB (모델이 학습할 때 RGB 모드로 학습했기 때문에 같은 형식으로 변환)\n",
    "    image_np = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # 0번 축을 늘리고 Tensor로 변환\n",
    "    input_tensor = tf.convert_to_tensor(image_np[np.newaxis, ...], dtype=tf.float32)\n",
    "    \n",
    "    # 추론\n",
    "    post_detection = detect_func(input_tensor)  #전처리-추론-후처리\n",
    "    \n",
    "    num_detections = int(post_detection.pop('num_detections'))\n",
    "    # 추론한 결과들을 num_detections 개수(detection한 물체의 개수)만큼의 값만 남긴다. 결과가 Tensor로 반환되는 것을 ndarray로 변환\n",
    "    detections = {key:value[0, :num_detections].numpy() for key, value in post_detection.items()}\n",
    "\n",
    "    # 새로 구성한 결과 dictionary(detections)에 num_detections 값을 추가\n",
    "    detections['num_detections'] = num_detections\n",
    "    # detection_classes는 검출한 box의 class 값을 label encoding된 값으로 가진다. float32로 반환되는 것을 int로 변환 처리\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    MIN_CONF_THRESH = 0.5  #물체가 있을 Confidence score가 0.5 이상인 bounding box만 나오도록 함\n",
    "    image_np_with_detection = image_np.copy()  #detection한 원본 이미지의 카피본 생성\n",
    "    img = viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                    image_np_with_detection,  #추론할 원본 이미지,\n",
    "                    detections['detection_boxes'],  #bounding box 좌표\n",
    "                    detections['detection_classes'] + 1,  #bounding box 내의 물체 index(class확률에서 0은 첫번째 label, label map의 id는 1부터 시작하기 때문에 +1)\n",
    "                    detections['detection_scores'],  #bounding box 내의 물체가 있을 확률(confidence score)\n",
    "                    category_index,\n",
    "                    use_normalized_coordinates=True,  #bounding box의 좌표들이 normalize되었는지 여부\n",
    "                    max_boxes_to_draw=200,  #최대 몇개 박스를 칠 것인지(기본: 20)\n",
    "                    min_score_thresh=MIN_CONF_THRESH  #Confidence score가 얼마 이상인 bounding box만 나오도록 하겠다\n",
    "                    )\n",
    "\n",
    "    # 결과 image를 RGB => BGR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # 화면에 출력\n",
    "    cv2.imshow('frame', img)\n",
    "    if cv2.waitKey(0) == 27:  #esc를 입력하면\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## post processing 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('one.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = img[np.newaxis, ...]\n",
    "input_tensor = tf.convert_to_tensor(img, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_detection = detect_func(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['detection_boxes', 'detection_scores', 'detection_classes', 'num_detections', 'raw_detection_boxes', 'raw_detection_scores', 'detection_multiclass_scores', 'detection_anchor_indices'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_detection.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100.], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_detection['num_detections'].numpy()  #Tensor => ndarray\n",
    "# num_detections: 후처리 결과로 나온 bounding box의 개수. 전체 bounding box에서 confidence score순으로 내림차순한 뒤 NMS를 거쳐서 최종적으로 남은 bounding box의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 100, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_detection['detection_boxes'].shape\n",
    "# [1, 100, 4] => [추론한 이미지 개수, num_detections, 좌표]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
