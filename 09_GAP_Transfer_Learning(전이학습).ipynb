{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "551.4px",
        "left": "1166px",
        "right": "20px",
        "top": "120px",
        "width": "350px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "09_GAP_Transfer Learning(전이학습).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jJnd7dxeN07Q",
        "aUmT5reoN07Q",
        "ObhTZxYnN07W",
        "PT25aQBUN07Y"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1hwSpMKN07G"
      },
      "source": [
        "# GlobalAveragePooling (GAP)\n",
        "- Feature map의 채널별로 평균값을 추출 1 x 1 x channel 의 Feature map을 생성\n",
        "- `model.add(keras.layers.GlobalAveragePooling2D())`\n",
        "![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7bM0XaTN07J"
      },
      "source": [
        "- Feature Extraction layer에서 추출한 Feature map을 Classifier layer로 Flatten해서 전달하면 많은 연결노드와 파라미터가 필요하게된다. GAP를 사용하면 노드와 파라미터의 개수를 효과적으로 줄일 수 있다.\n",
        "- Feature map의 채널수가 많을 경우 GAP를 사용하는 것이 효과적이나 채널수가 적다면 Flatten을 사용하는 것이 좋다.\n",
        "![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "kkw9VfORN07L",
        "outputId": "2878176b-60c3-4365-c651-eff1fe620631"
      },
      "source": [
        "# 이미지 다운로드\n",
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1nBE3N2cXQGwD8JaD0JZ2LmFD-n3D5hVU'\n",
        "fname = 'cats_and_dogs_small.zip'\n",
        "gdown.download(url, fname, quiet=False)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nBE3N2cXQGwD8JaD0JZ2LmFD-n3D5hVU\n",
            "To: /content/cats_and_dogs_small.zip\n",
            "90.8MB [00:00, 139MB/s] \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cats_and_dogs_small.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7iCeMQgN07L"
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxJoYuYJN07M"
      },
      "source": [
        "## 압축 풀기\n",
        "!unzip -q ./cats_and_dogs_small.zip -d data/cats_and_dogs_small"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRd0ZdFsOFlR"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "def get_generators():\n",
        "    '''\n",
        "    train, validation, test generator를 생성해서 반환.\n",
        "    train generator는 image 변환 처리\n",
        "    '''\n",
        "    train_dir = './data/cats_and_dogs_small/train'\n",
        "    validation_dir = './data/cats_and_dogs_small/validation'\n",
        "    test_dir = './data/cats_and_dogs_small/test'\n",
        "    train_datagen = ImageDataGenerator(rescale=1/255,\n",
        "                                       rotation_range=40,\n",
        "                                       brightness_range=(0.7,1.3),\n",
        "                                       zoom_range=0.2,\n",
        "                                       horizontal_flip=True)\n",
        "    test_datagen = ImageDataGenerator(rescale=1/255) #validation/test에서 사용\n",
        "    # generator 들 생성\n",
        "    train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                        target_size=(150,150),\n",
        "                                                        batch_size=N_BATCHS,\n",
        "                                                        class_mode='binary')\n",
        "    val_generator = test_datagen.flow_from_directory(validation_dir,\n",
        "                                                        target_size=(150,150),\n",
        "                                                        batch_size=N_BATCHS,\n",
        "                                                        class_mode='binary')\n",
        "    test_generator = test_datagen.flow_from_directory(test_dir,\n",
        "                                                        target_size=(150,150),\n",
        "                                                        batch_size=N_BATCHS,\n",
        "                                                        class_mode='binary')\n",
        "    return train_generator, val_generator, test_generator"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM-fR1SWN07M"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RttdPO6OOuvg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8c-v6nuN07N"
      },
      "source": [
        "## Transfer learning (전이학습)\n",
        "- 큰 데이터 셋을 이용해 미리 학습된 pre-trained Model의 Weight를 사용하여 현재 하려는 예측 문제에 활용. \n",
        "- ### Convolution base(Feature Extraction 부분)만 활용\n",
        "    - Convolution base는 이미지에 나타나는 일반적인 특성을 파악하기 위한 부분이므로 재사용할 수 있다.\n",
        "    - Classifier 부분은 학습하려는 데이터셋의 class들에 맞게 변경 해야 하므로 재사용할 수 없다.\n",
        "- Pretrained Convlution layer의 활용 \n",
        "    - Feature extraction\n",
        "        - 학습시 학습되지 않고 Feature를 추출하는 역할만 한다.\n",
        "    - Fine tuning\n",
        "        - 학습시 Pretrained Covolution layer도 같이 학습해서 내 데이터셋에 맞춘다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2GoSdonN07N"
      },
      "source": [
        "## Feature extraction\n",
        "- 기존의 학습된 network에서 fully connected layer를 제외한 나머지 weight를 고정하고 새로운 목적에 맞는 fully connected layer를 추가하여 추가된 weight만 학습하는 방법\n",
        "- `tensorflow.keras.applications` module이 지원하는  image classification models\n",
        "    - (https://www.tensorflow.org/api_docs/python/tf/keras/applications)    \n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Neh-ucdgN07N"
      },
      "source": [
        "> ### ImageNet\n",
        ">    - 웹상에서 수집한 약 1500만장의 라벨링된 고해상도 이미지로 약 22,000개 카테고리로 구성된다.\n",
        "\n",
        "> ### ILSVRC(ImageNet Large Scale Visual Recognition Challenge) 대회\n",
        ">   - 2010년 부터 2017년 까지 진행된 컴퓨터 비전 경진대회.\n",
        ">   - ImageNet의 이미지중 **1000개 카테고리 약 120만장의 학습용이미지, 5만장의 검증 이미지, 15만장의 테스트 이미지를** 이용해 대회를 진행한다.\n",
        ">   - **2012년** CNN기반 딥러닝 알고리즘인 **AlexNet**이 2위와 큰 차이로 우승하며 이후 딥러닝 알고리즘이 대세가 되었다. 특히 2015년 우승한 ResNet은 0.036의 에러율을 보이며 우승했는데 이는 사람이 에러율이라 알려진 0.05 보다 높은 정확도였다.\n",
        ">   - ILSVRC에서 우승하거나 좋은 성적을 올린 모델들이 컴퓨터 비전분야 발전에 큰 역할을 해왔으며 이후 다양한 딥러닝 모델의 백본(backbone)으로 사용되고 있다.\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp-1Kdp0N07O"
      },
      "source": [
        "##  VGG16 모델\n",
        "- ImageNet ILSVRC Challenge 2014에서 2등한 모델로 Simonyan and Zisserman(Oxford Univ.)에 의해 제안\n",
        "    - VGGNet이 준우승을 하긴 했지만, 구조의 간결함과 사용의 편이성으로 인해 1등한 GoogLeNet보다 더 각광받았다\n",
        "- 단순한 구조로 지금까지 많이 사용.\n",
        "- 총 16개 layer로 구성됨.\n",
        "- 네트워크 깊이가 어떤 영향을 주는 지 연구 하기 위해 설계된 네트워크로 동일한 kernel size에 convolution의 개수를 늘리는 방식으로 구성됨.\n",
        "    - 11 layer, 13 layer, 16 layer, 19 layer 의 네트워크를 테스트함. \n",
        "    - 19 layer의 성능이 16 layer보다 크게 나아지지 않음\n",
        "- Filter의 수가 64, 128, 256, 512 두 배씩 커짐 \n",
        "- 항상 $3 \\times 3$ filter, Stride=1, same padding, $2\\times 2$ MaxPooling 사용\n",
        "    - 이전 AlexNet이 5 X 5 필터를 사용했는데 VGG16은 3 X 3 필터 두개를 쌓아 사용했다.\n",
        "        - 3 x 3 필터 두개를 쌓는 것이 5 x 5  하나는 사용하는 보다 더 적은 파라미터를 사용하며 성능이 더 좋았다.\n",
        "    - Feature map의 사이즈를 convolution layer가 아닌 Max Pooling 을 사용해 줄여줌.\n",
        "- VGG16의 단점은 마지막에 분류를 위해 Fully Connected Layer 3개를 붙여 파라미터 수가 너무 많아 졌다. 약 1억4천만 개의 parameter(가중치)중 1억 2천만개 정도가 Fully Connected Layer의 파라미터 임.\n",
        "![image-3.png](attachment:image-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yll5ToiIN07O"
      },
      "source": [
        "## ResNet (Residual Networks)\n",
        "- 이전 모델들과 비교해 shortcut connection기법을 이용해 Layer수를 획기적으로 늘린 CNN 모델로 ILSVRC 2015년 대회에서 우승을 차지함.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "- 레이어를 깊게 쌓으면 성능이 더 좋아 지지 않을까? 실제는 Test 셋 뿐만 아니라 Train Set에서도 성능이 나쁘게 나옴.\n",
        "- Train set에서도 성능이 나쁘게 나온 것은 최적화 문제로 보고, 레이어를 깊게 쌓으면 최적화 하기가 어렵다고 생각함. \n",
        "![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqpWtFmNN07P"
      },
      "source": [
        "### Idea\n",
        "![image-3.png](attachment:image-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2_LBjA5N07P"
      },
      "source": [
        "- 입력값을 그대로 출력하는 identity block 을 사용하면 성능이 떨어지지는 않는다.\n",
        "- 그럼 Convloution block을 identity block으로 만들면 최소한 성능은 떨어지지 않고 깊은 Layer를 쌓을 수 있지 않을까?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_ijzcqxN07P"
      },
      "source": [
        "### Solution\n",
        "- Residual block\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWN1Ckx1N07P"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJnd7dxeN07Q"
      },
      "source": [
        "- 기존 Layer들의 목표는 입력값인 X를 출력값인 Y로 최적의 매핑할 수 있는 함수 H(X)를 찾는 것이다. 그래서 H(X) – Y 가 최소값이 되는 방향으로 학습을 진행하면서 H(X)를 찾음. 그런데 레이어가 깊어지면서 최적화에 어려움으로 성능이 떨어지는 문제가 발생\n",
        "\n",
        "- ResNet은 layer를 통과해서 나온 값이 **입력값과 동일하게 만드는 것을 목표로 하는 Identity block을** 구성한다.\n",
        "- Identity block은 입력값 X를 레이어를 통과시켜서 나온 Y에 입력값 X를 더해서 합치도록 구성한다.\n",
        "\n",
        "$$\\large H(x) = F(x) + x\\\\x: input,\\;H(x): output,\\;F(x): layer통과값$$ \n",
        " \n",
        " \n",
        "- 목표는 $H(x)$(레이어통과한 값) 가 input인 x와 동일한 것이므로 F(x)를 0으로 만들기 위해 학습을 한다. \n",
        "- $F(x)$는 **잔차(Residual)**가 된다. 그리고 잔차인 $F(x)$가 0이 되도록 학습하는 방식이므로 Residual Learning이라고 한다.\n",
        "- 입력인 x를 직접 전달하는 것을 **shortcut connection** 또는 **identity mapping** 또는 **skip connection** 이라고 한다.\n",
        "    - 이 shortcut은 파라미터 없이 단순히 값을 더하는 구조이므로 연산량에 크게 영향이 없다.\n",
        "- 그리고 Residual을 찾는 레이어를 **Residual Block, Identity Block** 이라고 한다.      \n",
        "\n",
        "### 성능향상\n",
        "- $H(x) = F(x) + x$ 을 $x$에 대해 미분하면 최소한 1이므로 Gradient Vanishing 문제를 극복한다.\n",
        "- 잔차학습이라고 하지만 Residual block 은 Convolution Layer와 Activation Layer로 구성되어 있기 때문에 이 Layer를 통과한 Input으로 부터 Feature map을 추출하는 과정은 진행되며 레이어가 깊으므로 다양한 더욱 풍부한 특성들을 추출하게 되어 성능이 향상된다.\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUmT5reoN07Q"
      },
      "source": [
        "### ResNet 구조\n",
        "- Residual block들을 쌓는 구조\n",
        "    - 일반 Convolution Layer(backbone)을 먼저 쌓고 Identity(Residual) block들을 계속 쌓는다.\n",
        "- 모든 Identity block은 두개의 3X3 conv layer로 구성됨.\n",
        "- 일정 레이어 수별로 filter의 개수를 두배로 증가시키며 stride를 2로 하여 downsampling 함. (Pooling Layer는 Identity block의 시작과 마지막에만 적용)\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57wlBTKuN07R"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzq20zalN07R"
      },
      "source": [
        "## Pretrained Model 사용\n",
        "- tensorflow.keras.applications 패키지를 통해 제공\n",
        "- 모델이름이 클래스이름\n",
        "    - VGG16, ResNet153 등등\n",
        "- 생성자 매개변수\n",
        "    - `weights`: 모형의 학습된 weight. 기본값- 'imagenet'\n",
        "    - `include_top`: fully connected layer를 포함할지 여부. True 포함시킴, False: 포함 안 시킴\n",
        "    - `input_shape`: 사용자가 입력할 이미지의 크기 shape. 3D 텐서로 지정. (높이, 너비, 채널). 기본값: (224,224, 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z03MAcQYOwNU"
      },
      "source": [
        "from tensorflow.keras.applications import VGG16, ResNet50V2\n",
        "\n",
        "conv_base = VGG16(weights='imagenet', #imagenet 데이터셋이 학습된 파라미터 사용\n",
        "                  include_top=False, #Classification(Fully Connected Layer)은 가져오지 않음\n",
        "                  input_shape=(150, 150, 3))\n",
        "\n",
        "# conv_base = ResNet50V2(weights='imagenet',\n",
        "#                        include_top=False,\n",
        "#                        input_shape=(150,150,3))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SllLGdmdO0iS",
        "outputId": "60550e39-d8b7-4652-ce71-3d18f93c926e"
      },
      "source": [
        "conv_base.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD-CvXLrO0n_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSwtGO-yO0uP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l6wzTzUOwqo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMCumLMON07R"
      },
      "source": [
        "## Feature extraction의 두 가지 방법\n",
        "1. **빠른 추출방식**\n",
        "    - 예측하려는 새로운 데이터를 위의 `conv_base`에 입력하여 나온 출력값을 numpy 배열로 저장하고 이를 분류 모델의 입력값으로 사용. Convolution operation을 하지 않아도 되기 때문에 빠르게 학습. 하지만 data augmentation 방법을 사용할 수 없음.\n",
        "\n",
        "2. **받아온 특성 Layer를 이용해 새로운 모델 구현하는 방식**\n",
        "    - 위의 `conv_base` 이후에 새로운 layer를 쌓아 확장한 뒤 전체 모델을 다시 학습. 모든 데이터가 convolution layer들을 통과해야 하기 때문에 학습이 느림. 단 conv_base의 가중치는 업데이트 되지 않도록 한다. data augmentation 방법을 사용할 수 있음."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHru8vA5N07S"
      },
      "source": [
        "### 빠른 특성 추출 방식\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6kjC6ZON07S"
      },
      "source": [
        "- `conv_base`의 predict 메소드로 입력 이미지의 feature를 추출 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy4cvq5oN07S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95mnFLEKN07S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21wtuffzN07T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuViPKEbN07T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x3s4JUHN07T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgiDShR7N07T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czD_yd54N07T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kHUuF72N07T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HpIaueGN07U"
      },
      "source": [
        "### Pretrained Network를 이용해 새로운 모델 구현하는 방식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pyj5HAaN07U"
      },
      "source": [
        "- Conv_base의 feature extraction 부분에 fully connected layer를 추가하여 모형 생성 \n",
        "- Conv_base에서 가져온 부분은 학습을 하지 않고 weight를 고정\n",
        "    -  **Layer.trainable=False**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYiIxlpzN07U"
      },
      "source": [
        "LEARNING_RATE = 0.001\n",
        "N_EPOCHS = 20\n",
        "N_BATCHS = 100\n",
        "IMAGE_SIZE = 150"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rel04hDhN07U"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def create_model():\n",
        "    conv_base = VGG16(weights='imagenet',\n",
        "                      include_top=False,\n",
        "                      input_shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n",
        "    \n",
        "    conv_base.trainable = False #학습 시 weight 최적화(update)되지 않도록 설정\n",
        "    #모델 컴파일 전에 실행해야 함\n",
        "\n",
        "    model = keras.Sequential()\n",
        "    model.add(conv_base)\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "\n",
        "    #출력\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bUCMcvTN07V",
        "outputId": "f52de01d-e4d0-4824-9121-6520326e41c1"
      },
      "source": [
        "model = create_model()\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "                                              loss='binary_crossentropy',\n",
        "                                              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 4, 4, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_3 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 14,846,273\n",
            "Trainable params: 131,585\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ask1Iv6aN07V",
        "outputId": "4b466ce4-d65d-4471-da3c-572f28859dd4"
      },
      "source": [
        "train_iterator, validation_iterator, test_iterator = get_generators()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAApJ430N07V",
        "outputId": "1347a2c4-9c85-45b6-a695-abd0d40acb17"
      },
      "source": [
        "history = model.fit(train_iterator, \n",
        "                    epochs=N_EPOCHS,\n",
        "                    steps_per_epoch=len(train_iterator),\n",
        "                    validation_data=validation_iterator,\n",
        "                    validation_steps=len(validation_iterator))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "20/20 [==============================] - 67s 1s/step - loss: 0.6339 - accuracy: 0.6400 - val_loss: 0.4401 - val_accuracy: 0.8340\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.4673 - accuracy: 0.7976 - val_loss: 0.3498 - val_accuracy: 0.8600\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.4081 - accuracy: 0.8086 - val_loss: 0.3217 - val_accuracy: 0.8720\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.3896 - accuracy: 0.8351 - val_loss: 0.3019 - val_accuracy: 0.8800\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.3276 - accuracy: 0.8675 - val_loss: 0.2846 - val_accuracy: 0.8840\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.3495 - accuracy: 0.8364 - val_loss: 0.2759 - val_accuracy: 0.8860\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.3339 - accuracy: 0.8577 - val_loss: 0.2723 - val_accuracy: 0.8890\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.3257 - accuracy: 0.8534 - val_loss: 0.2895 - val_accuracy: 0.8810\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.3328 - accuracy: 0.8438 - val_loss: 0.2627 - val_accuracy: 0.8950\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.3013 - accuracy: 0.8672 - val_loss: 0.2595 - val_accuracy: 0.8910\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.3079 - accuracy: 0.8607 - val_loss: 0.2609 - val_accuracy: 0.8940\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.2861 - accuracy: 0.8747 - val_loss: 0.2590 - val_accuracy: 0.8950\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.2972 - accuracy: 0.8786 - val_loss: 0.2549 - val_accuracy: 0.8960\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.3015 - accuracy: 0.8719 - val_loss: 0.2631 - val_accuracy: 0.8960\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.2886 - accuracy: 0.8753 - val_loss: 0.2545 - val_accuracy: 0.9030\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.2858 - accuracy: 0.8737 - val_loss: 0.2668 - val_accuracy: 0.8890\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.2895 - accuracy: 0.8737 - val_loss: 0.2573 - val_accuracy: 0.9000\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.2895 - accuracy: 0.8797 - val_loss: 0.2534 - val_accuracy: 0.8990\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.2869 - accuracy: 0.8850 - val_loss: 0.2538 - val_accuracy: 0.8990\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.2727 - accuracy: 0.8854 - val_loss: 0.2544 - val_accuracy: 0.8970\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z07W6rKLN07V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBQoY2U4N07V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1jliIo9N07V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7LtNshsN07W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unZpQqbNN07W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pvz92GZ1N07W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvnuyikZN07W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gewuYKR6N07W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgmriEDiN07W"
      },
      "source": [
        "## 미세조정(Fine-tuning)\n",
        "- Pretrained 모델을 내가 학습시켜야 하는 데이터셋(Custom Dataset)에 재학습시키는 것을 fine tunning 이라고 한다.\n",
        "- 주어진 문제에 더 적합하도록 모델의 가중치들을 조정."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObhTZxYnN07W"
      },
      "source": [
        "### Fine tuning 전략\n",
        "![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nZdZTxON07X"
      },
      "source": [
        "- **세 전략 모두 classifier layer들은 train한다.**\n",
        "\n",
        "1. <span style=\"font-size:1.2em;font-weight:bold\">전체 모델을 전부 학습시킨다.(1번)</span>\n",
        "    - Pretrained 모델의 weight는 Feature extraction 의 초기 weight 역할을 한다.\n",
        "    - **Train dataset의 양이 많고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **낮은 경우** 적용.\n",
        "    - 학습에 시간이 많이 걸린다.\n",
        "2. <span style=\"font-size:1.2em;font-weight:bold\">Pretrained 모델 Bottom layer들(Input과 가까운 Layer들)은 고정시키고 Top layer의 일부를 재학습시킨다.(2번)</span>\n",
        "    - **Train dataset의 양이 많고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **높은 경우** 적용.\n",
        "    - **Train dataset의 양이 적고** Pretained 모델이 학습했던 dataset과 custom dataset의 class간의 유사성이 **낮은 경우** 적용\n",
        "3. <span style=\"font-size:1.2em;font-weight:bold\">Pretrained 모델 전체를 고정시키고 classifier layer들만 학습시킨다.(3번)</span>\n",
        "    - **Train dataset의 양이 적고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **높은 경우** 적용.\n",
        "  \n",
        "  \n",
        "> custom dataset: 내가 학습시키고자 하는 dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIZtw9bnN07X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ny7Og8uN07X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94fz9PNPN07X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RWEtniZN07X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYbYmfsDN07X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT25aQBUN07Y"
      },
      "source": [
        "### Pretrained 모델 Bottom layer들(Input과 가까운 Layer들)은 고정시키고 Top layer의 일부를 재학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a64W9Ju5N07Y"
      },
      "source": [
        "- Conv_base에서 가장 Top부분에 있는 레이어에 대해 fine-tuning.\n",
        "    - 앞의 layer들은 비교적 일반적이고 재사용 가능한 feature를 학습\n",
        "    - 너무 많은 parameter를 학습시키면 overfitting의 위험이 있음 (특히 새로운 데이터의 수가 적을 때)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s55ScGlN07Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYQNlVUoN07Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59SQw_kDN07Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxJgqmylN07Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54c27Fq_N07Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1vnO_NuN07Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKTvzNsTN07Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNe43mubN07Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Tz-2pkN07Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgoWq1flN07Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}