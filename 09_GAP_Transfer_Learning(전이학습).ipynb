{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "551.4px",
        "left": "1166px",
        "right": "20px",
        "top": "120px",
        "width": "350px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "09_GAP_Transfer Learning(전이학습).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "i8c-v6nuN07N",
        "y2GoSdonN07N",
        "Neh-ucdgN07N",
        "Qp-1Kdp0N07O",
        "Yll5ToiIN07O",
        "yqpWtFmNN07P",
        "7_ijzcqxN07P",
        "jJnd7dxeN07Q",
        "aUmT5reoN07Q",
        "rzq20zalN07R"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1hwSpMKN07G"
      },
      "source": [
        "# GlobalAveragePooling (GAP)\n",
        "- Feature map의 채널별로 평균값을 추출 1 x 1 x channel 의 Feature map을 생성\n",
        "- `model.add(keras.layers.GlobalAveragePooling2D())`\n",
        "![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7bM0XaTN07J"
      },
      "source": [
        "- Feature Extraction layer에서 추출한 Feature map을 Classifier layer로 Flatten해서 전달하면 많은 연결노드와 파라미터가 필요하게된다. GAP를 사용하면 노드와 파라미터의 개수를 효과적으로 줄일 수 있다.\n",
        "- Feature map의 채널수가 많을 경우 GAP를 사용하는 것이 효과적이나 채널수가 적다면 Flatten을 사용하는 것이 좋다.\n",
        "![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "kkw9VfORN07L",
        "outputId": "6b7839a7-a9ad-4785-a18c-e530bd20cade"
      },
      "source": [
        "# 이미지 다운로드\n",
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1nBE3N2cXQGwD8JaD0JZ2LmFD-n3D5hVU'\n",
        "fname = 'cats_and_dogs_small.zip'\n",
        "gdown.download(url, fname, quiet=False)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nBE3N2cXQGwD8JaD0JZ2LmFD-n3D5hVU\n",
            "To: /content/cats_and_dogs_small.zip\n",
            "90.8MB [00:00, 344MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cats_and_dogs_small.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7iCeMQgN07L"
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxJoYuYJN07M"
      },
      "source": [
        "## 압축 풀기\n",
        "!unzip -q ./cats_and_dogs_small.zip -d data/cats_and_dogs_small"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRd0ZdFsOFlR"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "def get_generators():\n",
        "    '''\n",
        "    train, validation, test generator를 생성해서 반환.\n",
        "    train generator는 image 변환 처리\n",
        "    '''\n",
        "    train_dir = './data/cats_and_dogs_small/train'\n",
        "    validation_dir = './data/cats_and_dogs_small/validation'\n",
        "    test_dir = './data/cats_and_dogs_small/test'\n",
        "    train_datagen = ImageDataGenerator(rescale=1/255,\n",
        "                                       rotation_range=40,\n",
        "                                       brightness_range=(0.7,1.3),\n",
        "                                       zoom_range=0.2,\n",
        "                                       horizontal_flip=True)\n",
        "    test_datagen = ImageDataGenerator(rescale=1/255) #validation/test에서 사용\n",
        "    # generator 들 생성\n",
        "    train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                        target_size=(150,150),\n",
        "                                                        batch_size=N_BATCHS,\n",
        "                                                        class_mode='binary')\n",
        "    val_generator = test_datagen.flow_from_directory(validation_dir,\n",
        "                                                        target_size=(150,150),\n",
        "                                                        batch_size=N_BATCHS,\n",
        "                                                        class_mode='binary')\n",
        "    test_generator = test_datagen.flow_from_directory(test_dir,\n",
        "                                                        target_size=(150,150),\n",
        "                                                        batch_size=N_BATCHS,\n",
        "                                                        class_mode='binary')\n",
        "    return train_generator, val_generator, test_generator"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM-fR1SWN07M"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RttdPO6OOuvg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8c-v6nuN07N"
      },
      "source": [
        "## Transfer learning (전이학습)\n",
        "- 큰 데이터 셋을 이용해 미리 학습된 pre-trained Model의 Weight를 사용하여 현재 하려는 예측 문제에 활용. \n",
        "- ### Convolution base(Feature Extraction 부분)만 활용\n",
        "    - Convolution base는 이미지에 나타나는 일반적인 특성을 파악하기 위한 부분이므로 재사용할 수 있다.\n",
        "    - Classifier 부분은 학습하려는 데이터셋의 class들에 맞게 변경 해야 하므로 재사용할 수 없다.\n",
        "- Pretrained Convlution layer의 활용 \n",
        "    - Feature extraction\n",
        "        - 학습시 학습되지 않고 Feature를 추출하는 역할만 한다.\n",
        "    - Fine tuning\n",
        "        - 학습시 Pretrained Covolution layer도 같이 학습해서 내 데이터셋에 맞춘다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2GoSdonN07N"
      },
      "source": [
        "## Feature extraction\n",
        "- 기존의 학습된 network에서 fully connected layer를 제외한 나머지 weight를 고정하고 새로운 목적에 맞는 fully connected layer를 추가하여 추가된 weight만 학습하는 방법\n",
        "- `tensorflow.keras.applications` module이 지원하는  image classification models\n",
        "    - (https://www.tensorflow.org/api_docs/python/tf/keras/applications)    \n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Neh-ucdgN07N"
      },
      "source": [
        "> ### ImageNet\n",
        ">    - 웹상에서 수집한 약 1500만장의 라벨링된 고해상도 이미지로 약 22,000개 카테고리로 구성된다.\n",
        "\n",
        "> ### ILSVRC(ImageNet Large Scale Visual Recognition Challenge) 대회\n",
        ">   - 2010년 부터 2017년 까지 진행된 컴퓨터 비전 경진대회.\n",
        ">   - ImageNet의 이미지중 **1000개 카테고리 약 120만장의 학습용이미지, 5만장의 검증 이미지, 15만장의 테스트 이미지를** 이용해 대회를 진행한다.\n",
        ">   - **2012년** CNN기반 딥러닝 알고리즘인 **AlexNet**이 2위와 큰 차이로 우승하며 이후 딥러닝 알고리즘이 대세가 되었다. 특히 2015년 우승한 ResNet은 0.036의 에러율을 보이며 우승했는데 이는 사람이 에러율이라 알려진 0.05 보다 높은 정확도였다.\n",
        ">   - ILSVRC에서 우승하거나 좋은 성적을 올린 모델들이 컴퓨터 비전분야 발전에 큰 역할을 해왔으며 이후 다양한 딥러닝 모델의 백본(backbone)으로 사용되고 있다.\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp-1Kdp0N07O"
      },
      "source": [
        "##  VGG16 모델\n",
        "- ImageNet ILSVRC Challenge 2014에서 2등한 모델로 Simonyan and Zisserman(Oxford Univ.)에 의해 제안\n",
        "    - VGGNet이 준우승을 하긴 했지만, 구조의 간결함과 사용의 편이성으로 인해 1등한 GoogLeNet보다 더 각광받았다\n",
        "- 단순한 구조로 지금까지 많이 사용.\n",
        "- 총 16개 layer로 구성됨.\n",
        "- 네트워크 깊이가 어떤 영향을 주는 지 연구 하기 위해 설계된 네트워크로 동일한 kernel size에 convolution의 개수를 늘리는 방식으로 구성됨.\n",
        "    - 11 layer, 13 layer, 16 layer, 19 layer 의 네트워크를 테스트함. \n",
        "    - 19 layer의 성능이 16 layer보다 크게 나아지지 않음\n",
        "- Filter의 수가 64, 128, 256, 512 두 배씩 커짐 \n",
        "- 항상 $3 \\times 3$ filter, Stride=1, same padding, $2\\times 2$ MaxPooling 사용\n",
        "    - 이전 AlexNet이 5 X 5 필터를 사용했는데 VGG16은 3 X 3 필터 두개를 쌓아 사용했다.\n",
        "        - 3 x 3 필터 두개를 쌓는 것이 5 x 5  하나는 사용하는 보다 더 적은 파라미터를 사용하며 성능이 더 좋았다.\n",
        "    - Feature map의 사이즈를 convolution layer가 아닌 Max Pooling 을 사용해 줄여줌.\n",
        "- VGG16의 단점은 마지막에 분류를 위해 Fully Connected Layer 3개를 붙여 파라미터 수가 너무 많아 졌다. 약 1억4천만 개의 parameter(가중치)중 1억 2천만개 정도가 Fully Connected Layer의 파라미터 임.\n",
        "![image-3.png](attachment:image-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yll5ToiIN07O"
      },
      "source": [
        "## ResNet (Residual Networks)\n",
        "- 이전 모델들과 비교해 shortcut connection기법을 이용해 Layer수를 획기적으로 늘린 CNN 모델로 ILSVRC 2015년 대회에서 우승을 차지함.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "- 레이어를 깊게 쌓으면 성능이 더 좋아 지지 않을까? 실제는 Test 셋 뿐만 아니라 Train Set에서도 성능이 나쁘게 나옴.\n",
        "- Train set에서도 성능이 나쁘게 나온 것은 최적화 문제로 보고, 레이어를 깊게 쌓으면 최적화 하기가 어렵다고 생각함. \n",
        "![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqpWtFmNN07P"
      },
      "source": [
        "### Idea\n",
        "![image-3.png](attachment:image-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2_LBjA5N07P"
      },
      "source": [
        "- 입력값을 그대로 출력하는 identity block 을 사용하면 성능이 떨어지지는 않는다.\n",
        "- 그럼 Convloution block을 identity block으로 만들면 최소한 성능은 떨어지지 않고 깊은 Layer를 쌓을 수 있지 않을까?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_ijzcqxN07P"
      },
      "source": [
        "### Solution\n",
        "- Residual block\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWN1Ckx1N07P"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJnd7dxeN07Q"
      },
      "source": [
        "- 기존 Layer들의 목표는 입력값인 X를 출력값인 Y로 최적의 매핑할 수 있는 함수 H(X)를 찾는 것이다. 그래서 H(X) – Y 가 최소값이 되는 방향으로 학습을 진행하면서 H(X)를 찾음. 그런데 레이어가 깊어지면서 최적화에 어려움으로 성능이 떨어지는 문제가 발생\n",
        "\n",
        "- ResNet은 layer를 통과해서 나온 값이 **입력값과 동일하게 만드는 것을 목표로 하는 Identity block을** 구성한다.\n",
        "- Identity block은 입력값 X를 레이어를 통과시켜서 나온 Y에 입력값 X를 더해서 합치도록 구성한다.\n",
        "\n",
        "$$\\large H(x) = F(x) + x\\\\x: input,\\;H(x): output,\\;F(x): layer통과값$$ \n",
        " \n",
        " \n",
        "- 목표는 $H(x)$(레이어통과한 값) 가 input인 x와 동일한 것이므로 F(x)를 0으로 만들기 위해 학습을 한다. \n",
        "- $F(x)$는 **잔차(Residual)**가 된다. 그리고 잔차인 $F(x)$가 0이 되도록 학습하는 방식이므로 Residual Learning이라고 한다.\n",
        "- 입력인 x를 직접 전달하는 것을 **shortcut connection** 또는 **identity mapping** 또는 **skip connection** 이라고 한다.\n",
        "    - 이 shortcut은 파라미터 없이 단순히 값을 더하는 구조이므로 연산량에 크게 영향이 없다.\n",
        "- 그리고 Residual을 찾는 레이어를 **Residual Block, Identity Block** 이라고 한다.      \n",
        "\n",
        "### 성능향상\n",
        "- $H(x) = F(x) + x$ 을 $x$에 대해 미분하면 최소한 1이므로 Gradient Vanishing 문제를 극복한다.\n",
        "- 잔차학습이라고 하지만 Residual block 은 Convolution Layer와 Activation Layer로 구성되어 있기 때문에 이 Layer를 통과한 Input으로 부터 Feature map을 추출하는 과정은 진행되며 레이어가 깊으므로 다양한 더욱 풍부한 특성들을 추출하게 되어 성능이 향상된다.\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUmT5reoN07Q"
      },
      "source": [
        "### ResNet 구조\n",
        "- Residual block들을 쌓는 구조\n",
        "    - 일반 Convolution Layer(backbone)을 먼저 쌓고 Identity(Residual) block들을 계속 쌓는다.\n",
        "- 모든 Identity block은 두개의 3X3 conv layer로 구성됨.\n",
        "- 일정 레이어 수별로 filter의 개수를 두배로 증가시키며 stride를 2로 하여 downsampling 함. (Pooling Layer는 Identity block의 시작과 마지막에만 적용)\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57wlBTKuN07R"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzq20zalN07R"
      },
      "source": [
        "## Pretrained Model 사용\n",
        "- tensorflow.keras.applications 패키지를 통해 제공\n",
        "- 모델이름이 클래스이름\n",
        "    - VGG16, ResNet153 등등\n",
        "- 생성자 매개변수\n",
        "    - `weights`: 모형의 학습된 weight. 기본값- 'imagenet'\n",
        "    - `include_top`: fully connected layer를 포함할지 여부. True 포함시킴, False: 포함 안 시킴\n",
        "    - `input_shape`: 사용자가 입력할 이미지의 크기 shape. 3D 텐서로 지정. (높이, 너비, 채널). 기본값: (224,224, 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z03MAcQYOwNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e11e33fc-67aa-4a50-d074-3206c899bb9e"
      },
      "source": [
        "from tensorflow.keras.applications import VGG16, ResNet50V2\n",
        "\n",
        "conv_base = VGG16(weights='imagenet', #imagenet 데이터셋이 학습된 파라미터 사용\n",
        "                  include_top=False, #Classification(Fully Connected Layer)은 가져오지 않음\n",
        "                  input_shape=(150, 150, 3))\n",
        "\n",
        "# conv_base = ResNet50V2(weights='imagenet',\n",
        "#                        include_top=False,\n",
        "#                        input_shape=(150,150,3))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SllLGdmdO0iS",
        "outputId": "84d0b5ee-4084-4718-e8a5-7255bf0ec6bf"
      },
      "source": [
        "conv_base.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD-CvXLrO0n_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSwtGO-yO0uP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l6wzTzUOwqo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMCumLMON07R"
      },
      "source": [
        "## Feature extraction의 두 가지 방법\n",
        "1. **빠른 추출방식**\n",
        "    - 예측하려는 새로운 데이터를 위의 `conv_base`에 입력하여 나온 출력값을 numpy 배열로 저장하고 이를 분류 모델의 입력값으로 사용. Convolution operation을 하지 않아도 되기 때문에 빠르게 학습. 하지만 data augmentation 방법을 사용할 수 없음.\n",
        "\n",
        "2. **받아온 특성 Layer를 이용해 새로운 모델 구현하는 방식**\n",
        "    - 위의 `conv_base` 이후에 새로운 layer를 쌓아 확장한 뒤 전체 모델을 다시 학습. 모든 데이터가 convolution layer들을 통과해야 하기 때문에 학습이 느림. 단 conv_base의 가중치는 업데이트 되지 않도록 한다. data augmentation 방법을 사용할 수 있음."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHru8vA5N07S"
      },
      "source": [
        "### 빠른 특성 추출 방식\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6kjC6ZON07S"
      },
      "source": [
        "- `conv_base`의 predict 메소드로 입력 이미지의 feature를 추출 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy4cvq5oN07S"
      },
      "source": [
        "# 하이퍼파라미터\n",
        "LEARNING_RATE = 0.001\n",
        "N_EPOCHS = 30\n",
        "N_BATCHS = 100"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95mnFLEKN07S"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import VGG16, ResNet50V2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21wtuffzN07T"
      },
      "source": [
        "def extract_featuremap(image_directory, sample_counts):\n",
        "    \"\"\"\n",
        "    매개변수로 받은 디렉토리의 이미지들을 Conv_base 모델에 통과시켜 Featuremap들을 추출해 반환\n",
        "    \n",
        "    [매개변수]\n",
        "        image_directory: 이미지 데이터들이 있는 디렉토리\n",
        "        sample_counts: 특성을 추출할 이미지 개수\n",
        "    [반환값]\n",
        "        tuple (featuremap들, label)\n",
        "    \"\"\"\n",
        "\n",
        "    conv_base = VGG16(weights='imagenet',\n",
        "                      include_top=False,\n",
        "                      input_shape=(150,150,3))\n",
        "    # 결과를 담을 ndarray\n",
        "    return_features = np.zeros(shape=(sample_counts, 4,4,512)) #Featuremap 저장, conv_base의 마지막 layer의 output shape에 맞춤\n",
        "    return_labels = np.zeros(shape=(sample_counts, )) #label 저장\n",
        "\n",
        "    datagen = ImageDataGenerator(rescale=1./255)\n",
        "    iterator = datagen.flow_from_directory(image_directory,\n",
        "                                           target_size=(150,150),\n",
        "                                           batch_size=N_BATCHS,\n",
        "                                           class_mode='binary')\n",
        "\n",
        "    i = 0 #반복횟수 저장할 변수\n",
        "    for input_batch, label_batch in iterator: #(image, label) * batch크기(100)\n",
        "        #input_batch를 conv_base에 넣어서 featuremap 추출\n",
        "        fm = conv_base.predict(input_batch)\n",
        "\n",
        "        return_features[i*N_BATCHS: (i+1)*N_BATCHS] = fm\n",
        "        return_labels[i*N_BATCHS: (i+1)*N_BATCHS] = label_batch\n",
        "\n",
        "        i += 1\n",
        "        if i*N_BATCHS >= sample_counts: #결과를 저장할 배열의 시작 index가 sample_counts보다 크면 반복문 멈춤 \n",
        "            break\n",
        "\n",
        "    return return_features, return_labels"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuViPKEbN07T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "641c8083-0412-40fe-83da-e39fd4205c2f"
      },
      "source": [
        "train_dir = '/content/data/cats_and_dogs_small/train'\n",
        "validation_dir = '/content/data/cats_and_dogs_small/validation'\n",
        "test_dir = '/content/data/cats_and_dogs_small/test'\n",
        "\n",
        "#Featuremap 추출\n",
        "train_features, train_labels = extract_featuremap(train_dir, 2000)\n",
        "validation_features, validation_labels = extract_featuremap(validation_dir, 1000)\n",
        "test_features, test_labels = extract_featuremap(test_dir, 1000)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQmQQtYQUqO3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x3s4JUHN07T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8674e6f8-24d8-4079-b221-8e164d1ac52e"
      },
      "source": [
        "train_features.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 4, 4, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgiDShR7N07T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a96e1619-be9c-42e1-db88-dc31f1e0545f"
      },
      "source": [
        "train_labels.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czD_yd54N07T"
      },
      "source": [
        "def create_model():\n",
        "    # 분류기 모델만 생성\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input((4,4,512)))\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kHUuF72N07T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2990ae0d-6800-4539-9026-930c6b068c0c"
      },
      "source": [
        "model = create_model()\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "global_average_pooling2d_1 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 131,585\n",
            "Trainable params: 131,585\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv9yilciXHUq",
        "outputId": "11d2c693-388c-41d7-e517-499e4024aa6c"
      },
      "source": [
        "history = model.fit(train_features, train_labels,\n",
        "                    epochs=N_EPOCHS,\n",
        "                    validation_data=(validation_features, validation_labels),\n",
        "                    batch_size=N_BATCHS)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "20/20 [==============================] - 1s 14ms/step - loss: 0.6361 - accuracy: 0.6098 - val_loss: 0.4435 - val_accuracy: 0.8340\n",
            "Epoch 2/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.4188 - accuracy: 0.8094 - val_loss: 0.3482 - val_accuracy: 0.8680\n",
            "Epoch 3/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.3368 - accuracy: 0.8609 - val_loss: 0.3097 - val_accuracy: 0.8810\n",
            "Epoch 4/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2925 - accuracy: 0.8859 - val_loss: 0.2845 - val_accuracy: 0.8870\n",
            "Epoch 5/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2721 - accuracy: 0.8792 - val_loss: 0.2724 - val_accuracy: 0.8910\n",
            "Epoch 6/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2575 - accuracy: 0.9007 - val_loss: 0.2637 - val_accuracy: 0.8930\n",
            "Epoch 7/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2479 - accuracy: 0.8940 - val_loss: 0.2583 - val_accuracy: 0.8910\n",
            "Epoch 8/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2501 - accuracy: 0.8944 - val_loss: 0.2702 - val_accuracy: 0.8850\n",
            "Epoch 9/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2184 - accuracy: 0.9153 - val_loss: 0.3048 - val_accuracy: 0.8700\n",
            "Epoch 10/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2176 - accuracy: 0.9059 - val_loss: 0.2600 - val_accuracy: 0.8920\n",
            "Epoch 11/30\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1946 - accuracy: 0.9240 - val_loss: 0.2536 - val_accuracy: 0.8940\n",
            "Epoch 12/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1983 - accuracy: 0.9209 - val_loss: 0.2700 - val_accuracy: 0.8840\n",
            "Epoch 13/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1922 - accuracy: 0.9218 - val_loss: 0.2586 - val_accuracy: 0.8930\n",
            "Epoch 14/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1620 - accuracy: 0.9401 - val_loss: 0.2530 - val_accuracy: 0.8950\n",
            "Epoch 15/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1802 - accuracy: 0.9295 - val_loss: 0.2560 - val_accuracy: 0.8910\n",
            "Epoch 16/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1645 - accuracy: 0.9436 - val_loss: 0.2516 - val_accuracy: 0.8960\n",
            "Epoch 17/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1753 - accuracy: 0.9272 - val_loss: 0.2530 - val_accuracy: 0.8980\n",
            "Epoch 18/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1579 - accuracy: 0.9461 - val_loss: 0.2658 - val_accuracy: 0.8900\n",
            "Epoch 19/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1594 - accuracy: 0.9377 - val_loss: 0.2814 - val_accuracy: 0.8810\n",
            "Epoch 20/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1498 - accuracy: 0.9406 - val_loss: 0.2631 - val_accuracy: 0.8940\n",
            "Epoch 21/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1600 - accuracy: 0.9393 - val_loss: 0.2628 - val_accuracy: 0.8950\n",
            "Epoch 22/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1394 - accuracy: 0.9435 - val_loss: 0.2734 - val_accuracy: 0.8880\n",
            "Epoch 23/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1522 - accuracy: 0.9375 - val_loss: 0.2743 - val_accuracy: 0.8880\n",
            "Epoch 24/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1521 - accuracy: 0.9474 - val_loss: 0.2649 - val_accuracy: 0.8960\n",
            "Epoch 25/30\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1314 - accuracy: 0.9506 - val_loss: 0.2719 - val_accuracy: 0.8880\n",
            "Epoch 26/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1320 - accuracy: 0.9521 - val_loss: 0.2791 - val_accuracy: 0.8920\n",
            "Epoch 27/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1242 - accuracy: 0.9559 - val_loss: 0.2799 - val_accuracy: 0.8900\n",
            "Epoch 28/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1261 - accuracy: 0.9581 - val_loss: 0.2886 - val_accuracy: 0.8860\n",
            "Epoch 29/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1204 - accuracy: 0.9578 - val_loss: 0.2922 - val_accuracy: 0.8840\n",
            "Epoch 30/30\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1070 - accuracy: 0.9642 - val_loss: 0.2786 - val_accuracy: 0.8940\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3EXT5NZXHdo"
      },
      "source": [
        "# 한 개 이미지 추론\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "def predict_cat_dog(path, model, mode=False):\n",
        "    class_name = ['고양이', '강아지']\n",
        "    img = load_img(path, target_size=(150,150,3))\n",
        "    sample = img_to_array(img)[np.newaxis, ...]\n",
        "    sample = sample/255.\n",
        "    if mode: #conv_base를 거치도록\n",
        "        cb = VGG16(include_top=False, weights='imagenet', input_shape=(150,150,3))\n",
        "        sample = cb.predict(sample)\n",
        "    pred = model.predict(sample)\n",
        "    pred_class = np.where(pred < 0.5, 0, 1)\n",
        "    pred_class_name = class_name[pred_class[0,0]]\n",
        "\n",
        "    return pred, pred_class, pred_class_name"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgJQq98oZPoa",
        "outputId": "0b4285c6-467d-43e6-e8b8-9428090db565"
      },
      "source": [
        "predict_cat_dog('/content/dog.jpg', model, mode=True)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1.]], dtype=float32), array([[1]]), '강아지')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZuBN2jlXHkW",
        "outputId": "3a6d6d26-26bd-4b83-af66-c8a2c5f6a7b2"
      },
      "source": [
        "predict_cat_dog('/content/cat.jpg', model, mode=True)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 45 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb2459abdd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.0018874]], dtype=float32), array([[0]]), '고양이')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCZFfGr3bLsv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_8YRXRfbL3v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HpIaueGN07U"
      },
      "source": [
        "### Pretrained Network를 이용해 새로운 모델 구현하는 방식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pyj5HAaN07U"
      },
      "source": [
        "- Conv_base의 feature extraction 부분에 fully connected layer를 추가하여 모형 생성 \n",
        "- Conv_base에서 가져온 부분은 학습을 하지 않고 weight를 고정\n",
        "    -  **Layer.trainable=False**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYiIxlpzN07U"
      },
      "source": [
        "LEARNING_RATE = 0.001\n",
        "N_EPOCHS = 20\n",
        "N_BATCHS = 100\n",
        "IMAGE_SIZE = 150"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rel04hDhN07U"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def create_model():\n",
        "    conv_base = VGG16(weights='imagenet',\n",
        "                      include_top=False,\n",
        "                      input_shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n",
        "    \n",
        "    conv_base.trainable = False #학습 시 weight 최적화(update)되지 않도록 설정\n",
        "    #모델 컴파일 전에 실행해야 함\n",
        "\n",
        "    model = keras.Sequential()\n",
        "    model.add(conv_base)\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "\n",
        "    #출력\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bUCMcvTN07V",
        "outputId": "47c9b02f-55ac-4ae9-b09c-18a2218a52d8"
      },
      "source": [
        "model = create_model()\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "                                              loss='binary_crossentropy',\n",
        "                                              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 4, 4, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 14,846,273\n",
            "Trainable params: 131,585\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ask1Iv6aN07V",
        "outputId": "26953d13-5fba-49bf-a21c-9315b374862d"
      },
      "source": [
        "train_iterator, validation_iterator, test_iterator = get_generators()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAApJ430N07V",
        "outputId": "1347a2c4-9c85-45b6-a695-abd0d40acb17"
      },
      "source": [
        "history = model.fit(train_iterator, \n",
        "                    epochs=N_EPOCHS,\n",
        "                    steps_per_epoch=len(train_iterator),\n",
        "                    validation_data=validation_iterator,\n",
        "                    validation_steps=len(validation_iterator))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "20/20 [==============================] - 67s 1s/step - loss: 0.6339 - accuracy: 0.6400 - val_loss: 0.4401 - val_accuracy: 0.8340\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.4673 - accuracy: 0.7976 - val_loss: 0.3498 - val_accuracy: 0.8600\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.4081 - accuracy: 0.8086 - val_loss: 0.3217 - val_accuracy: 0.8720\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.3896 - accuracy: 0.8351 - val_loss: 0.3019 - val_accuracy: 0.8800\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.3276 - accuracy: 0.8675 - val_loss: 0.2846 - val_accuracy: 0.8840\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.3495 - accuracy: 0.8364 - val_loss: 0.2759 - val_accuracy: 0.8860\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.3339 - accuracy: 0.8577 - val_loss: 0.2723 - val_accuracy: 0.8890\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.3257 - accuracy: 0.8534 - val_loss: 0.2895 - val_accuracy: 0.8810\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.3328 - accuracy: 0.8438 - val_loss: 0.2627 - val_accuracy: 0.8950\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.3013 - accuracy: 0.8672 - val_loss: 0.2595 - val_accuracy: 0.8910\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.3079 - accuracy: 0.8607 - val_loss: 0.2609 - val_accuracy: 0.8940\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.2861 - accuracy: 0.8747 - val_loss: 0.2590 - val_accuracy: 0.8950\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.2972 - accuracy: 0.8786 - val_loss: 0.2549 - val_accuracy: 0.8960\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.3015 - accuracy: 0.8719 - val_loss: 0.2631 - val_accuracy: 0.8960\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.2886 - accuracy: 0.8753 - val_loss: 0.2545 - val_accuracy: 0.9030\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.2858 - accuracy: 0.8737 - val_loss: 0.2668 - val_accuracy: 0.8890\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.2895 - accuracy: 0.8737 - val_loss: 0.2573 - val_accuracy: 0.9000\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.2895 - accuracy: 0.8797 - val_loss: 0.2534 - val_accuracy: 0.8990\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.2869 - accuracy: 0.8850 - val_loss: 0.2538 - val_accuracy: 0.8990\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.2727 - accuracy: 0.8854 - val_loss: 0.2544 - val_accuracy: 0.8970\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z07W6rKLN07V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBQoY2U4N07V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1jliIo9N07V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7LtNshsN07W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unZpQqbNN07W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pvz92GZ1N07W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvnuyikZN07W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gewuYKR6N07W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgmriEDiN07W"
      },
      "source": [
        "## 미세조정(Fine-tuning)\n",
        "- Pretrained 모델을 내가 학습시켜야 하는 데이터셋(Custom Dataset)에 재학습시키는 것을 fine tunning 이라고 한다.\n",
        "- 주어진 문제에 더 적합하도록 모델의 가중치들을 조정."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObhTZxYnN07W"
      },
      "source": [
        "### Fine tuning 전략\n",
        "![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nZdZTxON07X"
      },
      "source": [
        "- **세 전략 모두 classifier layer들은 train한다.**\n",
        "\n",
        "1. <span style=\"font-size:1.2em;font-weight:bold\">전체 모델을 전부 학습시킨다.(1번)</span>\n",
        "    - Pretrained 모델의 weight는 Feature extraction 의 초기 weight 역할을 한다.\n",
        "    - **Train dataset의 양이 많고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **낮은 경우** 적용.\n",
        "    - 학습에 시간이 많이 걸린다.\n",
        "2. <span style=\"font-size:1.2em;font-weight:bold\">Pretrained 모델 Bottom layer들(Input과 가까운 Layer들)은 고정시키고 Top layer의 일부를 재학습시킨다.(2번)</span>\n",
        "    - **Train dataset의 양이 많고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **높은 경우** 적용.\n",
        "    - **Train dataset의 양이 적고** Pretained 모델이 학습했던 dataset과 custom dataset의 class간의 유사성이 **낮은 경우** 적용\n",
        "3. <span style=\"font-size:1.2em;font-weight:bold\">Pretrained 모델 전체를 고정시키고 classifier layer들만 학습시킨다.(3번)</span>\n",
        "    - **Train dataset의 양이 적고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **높은 경우** 적용.\n",
        "  \n",
        "  \n",
        "> custom dataset: 내가 학습시키고자 하는 dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIZtw9bnN07X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0cacfb8-c920-4d6a-9fee-8d0ea535da2b"
      },
      "source": [
        "cb = VGG16(weights='imagenet', include_top=False, input_shape=(150,150,3))\n",
        "cb.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ny7Og8uN07X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52012f61-8c84-4393-bc60-9c805c312e27"
      },
      "source": [
        "# network(모델)을 구성하는 layer들을 추출\n",
        "layers = cb.layers\n",
        "type(layers), len(layers) #model을 구성하는 layer들을 추출해서 list에 묶어 반환"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(list, 19)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94fz9PNPN07X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e8c14bd9-7102-405d-953d-2d5b8ab75be5"
      },
      "source": [
        "# layer의 이름\n",
        "layers[2].name"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'block1_conv2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RWEtniZN07X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70280d5-6369-4fbf-c6a8-a26d00d0a983"
      },
      "source": [
        "# 모델.get_layer('layer이름'): 지정한 이름의 layer를 반환\n",
        "l = cb.get_layer('block1_conv2')\n",
        "l"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f2ac7bb0b50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYbYmfsDN07X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e34ac847-5a19-4737-ad87-4a04b332b024"
      },
      "source": [
        "# layer의 가중치(weight) 조회 - layer객체.weights\n",
        "l_w = l.weights\n",
        "type(l_w), len(l_w) #(weights, bias)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(list, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3FycA_2-ZqJ",
        "outputId": "3814d283-80da-4054-f3fd-3256476081ec"
      },
      "source": [
        "np.shape(l_w[0]), np.shape(l_w[1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([3, 3, 64, 64]), TensorShape([64]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBLWX8du-Zw9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP3DVGK1-Z3O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2Qeth9B-Z8I"
      },
      "source": [
        "del layers"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT25aQBUN07Y"
      },
      "source": [
        "### Pretrained 모델 Bottom layer들(Input과 가까운 Layer들)은 고정시키고 Top layer의 일부를 재학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a64W9Ju5N07Y"
      },
      "source": [
        "- Conv_base에서 가장 Top부분에 있는 레이어에 대해 fine-tuning.\n",
        "    - 앞의 layer들은 비교적 일반적이고 재사용 가능한 feature를 학습\n",
        "    - 너무 많은 parameter를 학습시키면 overfitting의 위험이 있음 (특히 새로운 데이터의 수가 적을 때)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s55ScGlN07Y"
      },
      "source": [
        "def create_model():\n",
        "    #VGG16: block5_conv2, block5_conv3 두 개의 convolution layer들을 fine tuning\n",
        "    conv_base = VGG16(weights='imagenet',\n",
        "                      include_top=False,\n",
        "                      input_shape=(150,150,3))\n",
        "    \n",
        "    #trainable 설정\n",
        "    is_trainable = False\n",
        "    for layer in conv_base.layers:\n",
        "        if layer.name == 'block5_conv2':\n",
        "            is_trainable = True\n",
        "        layer.trainable = is_trainable\n",
        "\n",
        "    model = keras.Sequential()\n",
        "    model.add(conv_base)\n",
        "    model.add(keras.layers.GlobalAveragePooling2D())\n",
        "    model.add(keras.layers.Dense(256, activation='relu'))\n",
        "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYQNlVUoN07Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bfba5aa-0733-4e3c-82d1-ce0ec642a35f"
      },
      "source": [
        "model = create_model()\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "                                              loss='binary_crossentropy',\n",
        "                                              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 4, 4, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_3 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 14,846,273\n",
            "Trainable params: 4,851,201\n",
            "Non-trainable params: 9,995,072\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59SQw_kDN07Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264fa4b9-1ff3-4ff4-d3ac-0aa46814c942"
      },
      "source": [
        "train_iterator, validation_iterator, test_iterator = get_generators()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxJgqmylN07Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "357cf977-5a0e-460c-d2eb-de8f9f06ff71"
      },
      "source": [
        "N_EPOCHS = 30\n",
        "mc_callback = keras.callbacks.ModelCheckpoint('.models/cat_dog_model',\n",
        "                                              monitor='val_loss',\n",
        "                                              save_best_only=True)\n",
        "model.fit(train_iterator, epochs=N_EPOCHS,\n",
        "          steps_per_epoch=len(train_iterator),\n",
        "          validation_data=validation_iterator,\n",
        "          validation_steps=len(validation_iterator),\n",
        "          callbacks=[mc_callback])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "20/20 [==============================] - 60s 972ms/step - loss: 1.5230 - accuracy: 0.5242 - val_loss: 0.5794 - val_accuracy: 0.6720\n",
            "INFO:tensorflow:Assets written to: .models/cat_dog_model/assets\n",
            "Epoch 2/30\n",
            "20/20 [==============================] - 20s 977ms/step - loss: 0.5660 - accuracy: 0.6514 - val_loss: 0.3069 - val_accuracy: 0.8680\n",
            "INFO:tensorflow:Assets written to: .models/cat_dog_model/assets\n",
            "Epoch 3/30\n",
            "20/20 [==============================] - 20s 984ms/step - loss: 0.3544 - accuracy: 0.8562 - val_loss: 0.2435 - val_accuracy: 0.8940\n",
            "INFO:tensorflow:Assets written to: .models/cat_dog_model/assets\n",
            "Epoch 4/30\n",
            "20/20 [==============================] - 20s 984ms/step - loss: 0.2922 - accuracy: 0.8640 - val_loss: 0.2128 - val_accuracy: 0.9090\n",
            "INFO:tensorflow:Assets written to: .models/cat_dog_model/assets\n",
            "Epoch 5/30\n",
            "20/20 [==============================] - 20s 979ms/step - loss: 0.3214 - accuracy: 0.8594 - val_loss: 0.2270 - val_accuracy: 0.9060\n",
            "Epoch 6/30\n",
            "20/20 [==============================] - 19s 973ms/step - loss: 0.2575 - accuracy: 0.8940 - val_loss: 0.2022 - val_accuracy: 0.9180\n",
            "INFO:tensorflow:Assets written to: .models/cat_dog_model/assets\n",
            "Epoch 7/30\n",
            "20/20 [==============================] - 20s 983ms/step - loss: 0.2470 - accuracy: 0.8983 - val_loss: 0.2378 - val_accuracy: 0.8910\n",
            "Epoch 8/30\n",
            "20/20 [==============================] - 19s 971ms/step - loss: 0.2616 - accuracy: 0.8826 - val_loss: 0.2436 - val_accuracy: 0.8820\n",
            "Epoch 9/30\n",
            "20/20 [==============================] - 20s 983ms/step - loss: 0.2201 - accuracy: 0.9086 - val_loss: 0.2109 - val_accuracy: 0.9160\n",
            "Epoch 10/30\n",
            "20/20 [==============================] - 20s 983ms/step - loss: 0.2255 - accuracy: 0.9165 - val_loss: 0.1928 - val_accuracy: 0.9210\n",
            "INFO:tensorflow:Assets written to: .models/cat_dog_model/assets\n",
            "Epoch 11/30\n",
            "20/20 [==============================] - 20s 989ms/step - loss: 0.2066 - accuracy: 0.9119 - val_loss: 0.2171 - val_accuracy: 0.9090\n",
            "Epoch 12/30\n",
            "20/20 [==============================] - 20s 977ms/step - loss: 0.2076 - accuracy: 0.9118 - val_loss: 0.1791 - val_accuracy: 0.9210\n",
            "INFO:tensorflow:Assets written to: .models/cat_dog_model/assets\n",
            "Epoch 13/30\n",
            "20/20 [==============================] - 20s 990ms/step - loss: 0.1574 - accuracy: 0.9466 - val_loss: 0.1853 - val_accuracy: 0.9210\n",
            "Epoch 14/30\n",
            "20/20 [==============================] - 20s 987ms/step - loss: 0.1660 - accuracy: 0.9342 - val_loss: 0.1965 - val_accuracy: 0.9250\n",
            "Epoch 15/30\n",
            "20/20 [==============================] - 19s 974ms/step - loss: 0.1511 - accuracy: 0.9426 - val_loss: 0.1800 - val_accuracy: 0.9270\n",
            "Epoch 16/30\n",
            "20/20 [==============================] - 20s 993ms/step - loss: 0.1503 - accuracy: 0.9384 - val_loss: 0.1751 - val_accuracy: 0.9360\n",
            "INFO:tensorflow:Assets written to: .models/cat_dog_model/assets\n",
            "Epoch 17/30\n",
            "20/20 [==============================] - 20s 984ms/step - loss: 0.1272 - accuracy: 0.9491 - val_loss: 0.1872 - val_accuracy: 0.9200\n",
            "Epoch 18/30\n",
            "20/20 [==============================] - 19s 975ms/step - loss: 0.1438 - accuracy: 0.9410 - val_loss: 0.1912 - val_accuracy: 0.9280\n",
            "Epoch 19/30\n",
            "20/20 [==============================] - 19s 974ms/step - loss: 0.1071 - accuracy: 0.9643 - val_loss: 0.2050 - val_accuracy: 0.9180\n",
            "Epoch 20/30\n",
            "20/20 [==============================] - 19s 964ms/step - loss: 0.1282 - accuracy: 0.9474 - val_loss: 0.1881 - val_accuracy: 0.9330\n",
            "Epoch 21/30\n",
            "20/20 [==============================] - 19s 965ms/step - loss: 0.1031 - accuracy: 0.9549 - val_loss: 0.1695 - val_accuracy: 0.9410\n",
            "INFO:tensorflow:Assets written to: .models/cat_dog_model/assets\n",
            "Epoch 22/30\n",
            "20/20 [==============================] - 20s 982ms/step - loss: 0.0903 - accuracy: 0.9601 - val_loss: 0.1929 - val_accuracy: 0.9320\n",
            "Epoch 23/30\n",
            "20/20 [==============================] - 19s 975ms/step - loss: 0.1008 - accuracy: 0.9592 - val_loss: 0.2025 - val_accuracy: 0.9300\n",
            "Epoch 24/30\n",
            "20/20 [==============================] - 19s 974ms/step - loss: 0.0696 - accuracy: 0.9781 - val_loss: 0.1951 - val_accuracy: 0.9280\n",
            "Epoch 25/30\n",
            "20/20 [==============================] - 19s 962ms/step - loss: 0.0758 - accuracy: 0.9710 - val_loss: 0.2881 - val_accuracy: 0.9070\n",
            "Epoch 26/30\n",
            "20/20 [==============================] - 19s 969ms/step - loss: 0.1262 - accuracy: 0.9541 - val_loss: 0.2045 - val_accuracy: 0.9230\n",
            "Epoch 27/30\n",
            "20/20 [==============================] - 19s 967ms/step - loss: 0.0663 - accuracy: 0.9752 - val_loss: 0.1908 - val_accuracy: 0.9340\n",
            "Epoch 28/30\n",
            "20/20 [==============================] - 19s 965ms/step - loss: 0.0534 - accuracy: 0.9801 - val_loss: 0.2117 - val_accuracy: 0.9300\n",
            "Epoch 29/30\n",
            "20/20 [==============================] - 19s 970ms/step - loss: 0.0861 - accuracy: 0.9665 - val_loss: 0.1903 - val_accuracy: 0.9320\n",
            "Epoch 30/30\n",
            "20/20 [==============================] - 20s 979ms/step - loss: 0.0792 - accuracy: 0.9729 - val_loss: 0.2364 - val_accuracy: 0.9360\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2a5ee84050>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54c27Fq_N07Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "6f1c64e2-0283-434d-9555-c99daace0a87"
      },
      "source": [
        "best_model = keras.models.load_model('./models/cat_dog_model')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-06c66dac825c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./models/cat_dog_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    112\u001b[0m                   (export_dir,\n\u001b[1;32m    113\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: ./models/cat_dog_model/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1vnO_NuN07Z"
      },
      "source": [
        "# evaluation\n",
        "best_model.evaluate(train_iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKTvzNsTN07Z"
      },
      "source": [
        "best_model.evaluate(test_iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaJfNbokJtLT"
      },
      "source": [
        "predict_cat_dog('/content/cat.jpg', best_model, mode=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Tz-2pkN07Z"
      },
      "source": [
        "predict_cat_dog('/content/dog.jpg', best_model, mode=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgoWq1flN07Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}